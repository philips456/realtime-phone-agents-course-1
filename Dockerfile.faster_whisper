FROM ghcr.io/speaches-ai/speaches:latest-cuda

# Build argument for the model - can be overridden at build time
ARG FASTER_WHISPER_MODEL=Systran/faster-whisper-large-v3

# Expose the default Speaches port
EXPOSE 8000

# Set up the HuggingFace cache directory
ENV HF_HOME=/home/ubuntu/.cache/huggingface
ENV TRANSFORMERS_CACHE=/home/ubuntu/.cache/huggingface/hub

# Ensure the cache directory exists with proper permissions
RUN mkdir -p /home/ubuntu/.cache/huggingface/hub

# Pre-download the Faster Whisper model during build
# This uses huggingface-cli which is included in the base image
RUN echo "Downloading model: ${FASTER_WHISPER_MODEL}" && \
    huggingface-cli download ${FASTER_WHISPER_MODEL} --local-dir-use-symlinks False && \
    echo "âœ“ Model downloaded successfully!"

# Set runtime environment variables
ENV DEFAULT_MODEL=${FASTER_WHISPER_MODEL}
ENV COMPUTE_TYPE=int8
ENV LOOPBACK_HOST_URL=http://localhost:8000

# The base image already has the CMD configured
# Port 8000 is exposed and ready for RunPod deployment
